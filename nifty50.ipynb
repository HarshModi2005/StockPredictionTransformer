{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Add, GlobalAveragePooling1D\n",
    "\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bollinger_bands(data, window=10, num_of_std=2):\n",
    "    \"\"\"Calculate Bollinger Bands\"\"\"\n",
    "    rolling_mean = data.rolling(window=window).mean()\n",
    "    rolling_std = data.rolling(window=window).std()\n",
    "    upper_band = rolling_mean + (rolling_std * num_of_std)\n",
    "    lower_band = rolling_mean - (rolling_std * num_of_std)\n",
    "    return upper_band, lower_band\n",
    "\n",
    "def calculate_rsi(data, window=10):\n",
    "    \"\"\"Calculate Relative Strength Index\"\"\"\n",
    "    delta = data.diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "    avg_gain = gain.rolling(window=window, min_periods=1).mean()\n",
    "    avg_loss = loss.rolling(window=window, min_periods=1).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def calculate_roc(data, periods=10):\n",
    "    \"\"\"Calculate Rate of Change.\"\"\"\n",
    "    roc = ((data - data.shift(periods)) / data.shift(periods)) * 100\n",
    "    return roc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['MSFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_data_frames = []\n",
    "stats = {}\n",
    "for ticker in tickers:\n",
    "  \n",
    "    # Download historical data for the ticker\n",
    "    data = yf.download(ticker, period=\"max\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate the daily percentage change\n",
    "\n",
    "    open = data['Open']\n",
    "    close = data['Close']\n",
    "    upperClose, lowerClose = calculate_bollinger_bands(close, window=14, num_of_std=2)\n",
    "    upperOpen, lowerOpen = calculate_bollinger_bands(open, window=14, num_of_std=2)\n",
    "\n",
    "    #Close Parameters\n",
    "\n",
    "    widthClose = upperClose - lowerClose\n",
    "    rsiClose = calculate_rsi(close, window=14)\n",
    "    rocClose = calculate_roc(close, periods=14)\n",
    "    percent_change_close = data['Close'].pct_change() * 100\n",
    "    diffClose = data['Close'].diff(1)\n",
    "\n",
    "    #Open Parameters \n",
    "\n",
    "    widthOpen = upperOpen - lowerOpen\n",
    "    rsiOpen = calculate_rsi(open, window=14)\n",
    "    rocOpen = calculate_roc(open, periods=14)\n",
    "    percent_change_open= data['Open'].pct_change() * 100\n",
    "    diffOpen = data['Open'].diff(1)\n",
    "\n",
    "\n",
    "    volume = data['Volume']\n",
    "    \n",
    "    # Create a DataFrame for the current ticker and append it to the list\n",
    "    ticker_df = pd.DataFrame({\n",
    "        ticker+'_close': close,\n",
    "        ticker+'_open': open,\n",
    "        ticker+'_widthOpen': widthOpen,\n",
    "        ticker+'_widthClose': widthClose,\n",
    "\n",
    "        ticker+'_rsiClose': rsiClose,\n",
    "        ticker+'_rsiOpen': rsiOpen,\n",
    "        ticker+'_rocOpen': rocOpen,\n",
    "        ticker+'_rocClose': rocClose,\n",
    "        ticker+'_volume': volume,\n",
    "        ticker+'_diffOpen': diffOpen,\n",
    "        ticker+'_diffClose': diffClose,\n",
    "        ticker+'_percent_change_close': percent_change_close,\n",
    "        ticker+'_percent_change_open': percent_change_open\n",
    "    })\n",
    "    \n",
    "    MEAN = ticker_df.mean()\n",
    "    STD = ticker_df.std()\n",
    "\n",
    "    # Keep track of mean and std\n",
    "    for column in MEAN.index:\n",
    "      stats[f\"{column}_mean\"] = MEAN[column]\n",
    "      stats[f\"{column}_std\"] = STD[column]\n",
    "    \n",
    "    # Normalize the training features\n",
    "    ticker_df = (ticker_df - MEAN) / STD\n",
    "\n",
    "    ticker_data_frames.append(ticker_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSFT_close_mean</th>\n",
       "      <th>MSFT_close_std</th>\n",
       "      <th>MSFT_open_mean</th>\n",
       "      <th>MSFT_open_std</th>\n",
       "      <th>MSFT_widthOpen_mean</th>\n",
       "      <th>MSFT_widthOpen_std</th>\n",
       "      <th>MSFT_widthClose_mean</th>\n",
       "      <th>MSFT_widthClose_std</th>\n",
       "      <th>MSFT_rsiClose_mean</th>\n",
       "      <th>MSFT_rsiClose_std</th>\n",
       "      <th>...</th>\n",
       "      <th>MSFT_volume_mean</th>\n",
       "      <th>MSFT_volume_std</th>\n",
       "      <th>MSFT_diffOpen_mean</th>\n",
       "      <th>MSFT_diffOpen_std</th>\n",
       "      <th>MSFT_diffClose_mean</th>\n",
       "      <th>MSFT_diffClose_std</th>\n",
       "      <th>MSFT_percent_change_close_mean</th>\n",
       "      <th>MSFT_percent_change_close_std</th>\n",
       "      <th>MSFT_percent_change_open_mean</th>\n",
       "      <th>MSFT_percent_change_open_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57.207628</td>\n",
       "      <td>87.630568</td>\n",
       "      <td>57.191045</td>\n",
       "      <td>87.600946</td>\n",
       "      <td>5.103543</td>\n",
       "      <td>8.401971</td>\n",
       "      <td>5.179191</td>\n",
       "      <td>8.573239</td>\n",
       "      <td>53.932242</td>\n",
       "      <td>16.138387</td>\n",
       "      <td>...</td>\n",
       "      <td>5.689305e+07</td>\n",
       "      <td>3.814202e+07</td>\n",
       "      <td>0.045451</td>\n",
       "      <td>1.732641</td>\n",
       "      <td>0.045895</td>\n",
       "      <td>1.769363</td>\n",
       "      <td>0.109845</td>\n",
       "      <td>2.11461</td>\n",
       "      <td>0.110468</td>\n",
       "      <td>2.107235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSFT_close_mean  MSFT_close_std  MSFT_open_mean  MSFT_open_std  \\\n",
       "0        57.207628       87.630568       57.191045      87.600946   \n",
       "\n",
       "   MSFT_widthOpen_mean  MSFT_widthOpen_std  MSFT_widthClose_mean  \\\n",
       "0             5.103543            8.401971              5.179191   \n",
       "\n",
       "   MSFT_widthClose_std  MSFT_rsiClose_mean  MSFT_rsiClose_std  ...  \\\n",
       "0             8.573239           53.932242          16.138387  ...   \n",
       "\n",
       "   MSFT_volume_mean  MSFT_volume_std  MSFT_diffOpen_mean  MSFT_diffOpen_std  \\\n",
       "0      5.689305e+07     3.814202e+07            0.045451           1.732641   \n",
       "\n",
       "   MSFT_diffClose_mean  MSFT_diffClose_std  MSFT_percent_change_close_mean  \\\n",
       "0             0.045895            1.769363                        0.109845   \n",
       "\n",
       "   MSFT_percent_change_close_std  MSFT_percent_change_open_mean  \\\n",
       "0                        2.11461                       0.110468   \n",
       "\n",
       "   MSFT_percent_change_open_std  \n",
       "0                      2.107235  \n",
       "\n",
       "[1 rows x 26 columns]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the dictionary containing feature statistics to a DataFrame for easier access\n",
    "stats = pd.DataFrame(stats, index=[0])\n",
    "\n",
    "# Display the DataFrame to verify its structure\n",
    "stats.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(ticker_data_frames, axis=1)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift the dataframe up by one to align current features with the next step's outcomes\n",
    "labels = df.shift(-1)\n",
    "\n",
    "# Remove the last row from both the features and labels to maintain consistent data pairs\n",
    "df = df.iloc[:-1]\n",
    "labels = labels.iloc[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LEN = 1  # 2 hours of data at 5-minute intervals\n",
    "\n",
    "def create_sequences(data, labelsOpen, labelsClose, meanOpen,meanClose, stdOpen,stdClose, sequence_length=SEQUENCE_LEN):\n",
    "    sequences = []\n",
    "    labClose = []\n",
    "    labOpen = []\n",
    "    data_size = len(data)\n",
    "\n",
    "    # Loop to create each sequence and its corresponding label\n",
    "    for i in range(data_size - (sequence_length + sequence_length//2 + 1 )): # Ensure we have data for the label\n",
    "        if i == 0:\n",
    "          continue\n",
    "        sequences.append(data[i:i + sequence_length])  # The sequence of data\n",
    "        labClose.append([labelsClose[i-1], labelsClose[i + sequence_length//2],meanClose[0], stdClose[0]]) # The label and scaling factors\n",
    "        labOpen.append([labelsOpen[i-1], labelsOpen[i + sequence_length//2],meanOpen[0], stdOpen[0]])\n",
    "    return np.array(sequences), np.array(labClose) , np.array(labOpen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_dict = {}\n",
    "sequence_labels_open = {}\n",
    "sequence_labels_close = {}\n",
    "for ticker in tickers:\n",
    "\n",
    "    # Extract close and volume data for the ticker\n",
    "    close = df[ticker+'_close'].values\n",
    "    open = df[ticker+'_open'].values\n",
    "    widthOpen = df[ticker+'_widthOpen'].values\n",
    "    widthClose = df[ticker+'_widthClose'].values\n",
    "    rsiOpen = df[ticker+'_rsiOpen'].values\n",
    "    rsiClose = df[ticker+'_rsiClose'].values\n",
    "    rocOpen = df[ticker+'_rocOpen'].values\n",
    "    rocClose = df[ticker+'_rocClose'].values\n",
    "\n",
    "    # rsi = df[ticker+'_rsi'].values\n",
    "    # roc = df[ticker+'_roc'].values\n",
    "    volume = df[ticker+'_volume'].values\n",
    "\n",
    "    diffOpen = df[ticker+'_diffOpen'].values\n",
    "    diffClose = df[ticker+'_diffClose'].values\n",
    "\n",
    "    percent_change_close = df[ticker+'_percent_change_close'].values\n",
    "    percent_change_open = df[ticker+'_percent_change_open'].values\n",
    "\n",
    "    \n",
    "    # Combine close and volume data\n",
    "    ticker_data = np.column_stack((\n",
    "                                    open,\n",
    "                                   widthOpen,\n",
    "                                    rsiOpen,\n",
    "                                    \n",
    "                                    rocOpen,\n",
    "                                    \n",
    "                                    \n",
    "                                    diffOpen,\n",
    "                                    \n",
    "                                    \n",
    "                                    percent_change_open,\n",
    "                                   close,\n",
    "                                   widthClose,\n",
    "                                   rsiClose,\n",
    "                                   rocClose,\n",
    "                                   diffClose,\n",
    "                                   percent_change_close,\n",
    "                                   \n",
    "                                    volume\n",
    "                                    ))\n",
    "    \n",
    "                                   \n",
    "    \n",
    "    # Generate sequences\n",
    "    attributeClose = ticker+\"_close\"\n",
    "    attributeOpen = ticker+\"_open\"\n",
    "\n",
    "    ticker_sequences, labClose, labOpen = create_sequences(ticker_data,\n",
    "                                             labels[attributeOpen].values[SEQUENCE_LEN-1:],\n",
    "                                             labels[attributeClose].values[SEQUENCE_LEN-1:],\n",
    "                                             \n",
    "                                             stats[attributeOpen+\"_mean\"].values,\n",
    "                                             stats[attributeOpen+\"_std\"].values,\n",
    "                                             stats[attributeClose+\"_mean\"].values,\n",
    "                                             stats[attributeClose+\"_std\"].values,\n",
    "                                             \n",
    "                                             \n",
    "                                            \n",
    "                                             )\n",
    "    \n",
    "    sequences_dict[ticker] = ticker_sequences\n",
    "    sequence_labels_close[ticker] = labClose\n",
    "    sequence_labels_open[ticker] = labOpen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9624, 1, 13)\n"
     ]
    }
   ],
   "source": [
    "# Combine data and labels from all tickers\n",
    "all_sequences = []\n",
    "all_labelsOpen = []\n",
    "all_labelsClose = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    all_sequences.extend(sequences_dict[ticker])\n",
    "    all_labelsOpen.extend(sequence_labels_open[ticker])\n",
    "\n",
    "    all_labelsClose.extend(sequence_labels_close[ticker])\n",
    "    \n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_sequences = np.array(all_sequences)\n",
    "all_labels_open = np.array(all_labelsOpen)\n",
    "all_labels_close = np.array(all_labelsClose)\n",
    "print(all_sequences.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "shuffled_indices = np.random.permutation(len(all_sequences))\n",
    "all_sequences = all_sequences[shuffled_indices]\n",
    "all_labels_open = all_labels_open[shuffled_indices]\n",
    "all_labels_close = all_labels_close[shuffled_indices]\n",
    "\n",
    "train_size = int(len(all_sequences) * 0.9)\n",
    "\n",
    "# Split sequences\n",
    "train_sequences = all_sequences[:train_size]\n",
    "train_labels_open    = all_labels_open[:train_size]\n",
    "train_labels_close = all_labels_close[:train_size]\n",
    "\n",
    "other_sequences = all_sequences[train_size:]\n",
    "other_labels_open    = all_labels_open[train_size:]\n",
    "other_labels_close = all_labels_close[train_size:]\n",
    "\n",
    "shuffled_indices = np.random.permutation(len(other_sequences))\n",
    "other_sequences = other_sequences[shuffled_indices]\n",
    "other_labels_close = other_labels_close[shuffled_indices]\n",
    "other_labels_open = other_labels_open[shuffled_indices]\n",
    "\n",
    "val_size = int(len(other_sequences) * 0.5)\n",
    "\n",
    "validation_sequences = other_sequences[:val_size]\n",
    "validation_labels_open = other_labels_open[:val_size]\n",
    "validation_labels_close = other_labels_close[:val_size]\n",
    "\n",
    "test_sequences = other_sequences[val_size:]\n",
    "test_labels_open = other_labels_open[val_size:]\n",
    "test_labels_close = other_labels_close[val_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Attention and Normalization\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = Add()([x, inputs])\n",
    "\n",
    "    # Feed Forward Part\n",
    "    y = LayerNormalization(epsilon=1e-6)(x)\n",
    "    y = Dense(ff_dim, activation=\"relu\")(y)\n",
    "    y = Dropout(dropout)(y)\n",
    "    y = Dense(inputs.shape[-1])(y)\n",
    "    return Add()([y, x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_layers, dropout=0):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    outputs = Dense(1, activation=\"linear\")(x)\n",
    "    return Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 13)\n",
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_23 (InputLayer)       [(None, 1, 13)]              0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization_310 (L  (None, 1, 13)                26        ['input_23[0][0]']            \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_144 (  (None, 1, 13)                225293    ['layer_normalization_310[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_310[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_288 (Add)               (None, 1, 13)                0         ['multi_head_attention_144[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'input_23[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_311 (L  (None, 1, 13)                26        ['add_288[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_300 (Dense)           (None, 1, 1024)              14336     ['layer_normalization_311[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_144 (Dropout)       (None, 1, 1024)              0         ['dense_300[0][0]']           \n",
      "                                                                                                  \n",
      " dense_301 (Dense)           (None, 1, 13)                13325     ['dropout_144[0][0]']         \n",
      "                                                                                                  \n",
      " add_289 (Add)               (None, 1, 13)                0         ['dense_301[0][0]',           \n",
      "                                                                     'add_288[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_312 (L  (None, 1, 13)                26        ['add_289[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_145 (  (None, 1, 13)                225293    ['layer_normalization_312[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_312[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_290 (Add)               (None, 1, 13)                0         ['multi_head_attention_145[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_289[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_313 (L  (None, 1, 13)                26        ['add_290[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_302 (Dense)           (None, 1, 1024)              14336     ['layer_normalization_313[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_145 (Dropout)       (None, 1, 1024)              0         ['dense_302[0][0]']           \n",
      "                                                                                                  \n",
      " dense_303 (Dense)           (None, 1, 13)                13325     ['dropout_145[0][0]']         \n",
      "                                                                                                  \n",
      " add_291 (Add)               (None, 1, 13)                0         ['dense_303[0][0]',           \n",
      "                                                                     'add_290[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_314 (L  (None, 1, 13)                26        ['add_291[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_146 (  (None, 1, 13)                225293    ['layer_normalization_314[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_314[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_292 (Add)               (None, 1, 13)                0         ['multi_head_attention_146[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_291[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_315 (L  (None, 1, 13)                26        ['add_292[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_304 (Dense)           (None, 1, 1024)              14336     ['layer_normalization_315[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_146 (Dropout)       (None, 1, 1024)              0         ['dense_304[0][0]']           \n",
      "                                                                                                  \n",
      " dense_305 (Dense)           (None, 1, 13)                13325     ['dropout_146[0][0]']         \n",
      "                                                                                                  \n",
      " add_293 (Add)               (None, 1, 13)                0         ['dense_305[0][0]',           \n",
      "                                                                     'add_292[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_316 (L  (None, 1, 13)                26        ['add_293[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_147 (  (None, 1, 13)                225293    ['layer_normalization_316[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_316[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_294 (Add)               (None, 1, 13)                0         ['multi_head_attention_147[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_293[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_317 (L  (None, 1, 13)                26        ['add_294[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_306 (Dense)           (None, 1, 1024)              14336     ['layer_normalization_317[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_147 (Dropout)       (None, 1, 1024)              0         ['dense_306[0][0]']           \n",
      "                                                                                                  \n",
      " dense_307 (Dense)           (None, 1, 13)                13325     ['dropout_147[0][0]']         \n",
      "                                                                                                  \n",
      " add_295 (Add)               (None, 1, 13)                0         ['dense_307[0][0]',           \n",
      "                                                                     'add_294[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_318 (L  (None, 1, 13)                26        ['add_295[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_148 (  (None, 1, 13)                225293    ['layer_normalization_318[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_318[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_296 (Add)               (None, 1, 13)                0         ['multi_head_attention_148[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_295[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_319 (L  (None, 1, 13)                26        ['add_296[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_308 (Dense)           (None, 1, 1024)              14336     ['layer_normalization_319[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_148 (Dropout)       (None, 1, 1024)              0         ['dense_308[0][0]']           \n",
      "                                                                                                  \n",
      " dense_309 (Dense)           (None, 1, 13)                13325     ['dropout_148[0][0]']         \n",
      "                                                                                                  \n",
      " add_297 (Add)               (None, 1, 13)                0         ['dense_309[0][0]',           \n",
      "                                                                     'add_296[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_320 (L  (None, 1, 13)                26        ['add_297[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_149 (  (None, 1, 13)                225293    ['layer_normalization_320[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_320[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_298 (Add)               (None, 1, 13)                0         ['multi_head_attention_149[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_297[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_321 (L  (None, 1, 13)                26        ['add_298[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_310 (Dense)           (None, 1, 1024)              14336     ['layer_normalization_321[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_149 (Dropout)       (None, 1, 1024)              0         ['dense_310[0][0]']           \n",
      "                                                                                                  \n",
      " dense_311 (Dense)           (None, 1, 13)                13325     ['dropout_149[0][0]']         \n",
      "                                                                                                  \n",
      " add_299 (Add)               (None, 1, 13)                0         ['dense_311[0][0]',           \n",
      "                                                                     'add_298[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_322 (L  (None, 1, 13)                26        ['add_299[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_150 (  (None, 1, 13)                225293    ['layer_normalization_322[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_322[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_300 (Add)               (None, 1, 13)                0         ['multi_head_attention_150[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_299[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_323 (L  (None, 1, 13)                26        ['add_300[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_312 (Dense)           (None, 1, 1024)              14336     ['layer_normalization_323[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_150 (Dropout)       (None, 1, 1024)              0         ['dense_312[0][0]']           \n",
      "                                                                                                  \n",
      " dense_313 (Dense)           (None, 1, 13)                13325     ['dropout_150[0][0]']         \n",
      "                                                                                                  \n",
      " add_301 (Add)               (None, 1, 13)                0         ['dense_313[0][0]',           \n",
      "                                                                     'add_300[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_324 (L  (None, 1, 13)                26        ['add_301[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_151 (  (None, 1, 13)                225293    ['layer_normalization_324[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_324[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_302 (Add)               (None, 1, 13)                0         ['multi_head_attention_151[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_301[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_325 (L  (None, 1, 13)                26        ['add_302[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_314 (Dense)           (None, 1, 1024)              14336     ['layer_normalization_325[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_151 (Dropout)       (None, 1, 1024)              0         ['dense_314[0][0]']           \n",
      "                                                                                                  \n",
      " dense_315 (Dense)           (None, 1, 13)                13325     ['dropout_151[0][0]']         \n",
      "                                                                                                  \n",
      " add_303 (Add)               (None, 1, 13)                0         ['dense_315[0][0]',           \n",
      "                                                                     'add_302[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_326 (L  (None, 1, 13)                26        ['add_303[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_152 (  (None, 1, 13)                225293    ['layer_normalization_326[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_326[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_304 (Add)               (None, 1, 13)                0         ['multi_head_attention_152[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_303[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_327 (L  (None, 1, 13)                26        ['add_304[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_316 (Dense)           (None, 1, 1024)              14336     ['layer_normalization_327[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_152 (Dropout)       (None, 1, 1024)              0         ['dense_316[0][0]']           \n",
      "                                                                                                  \n",
      " dense_317 (Dense)           (None, 1, 13)                13325     ['dropout_152[0][0]']         \n",
      "                                                                                                  \n",
      " add_305 (Add)               (None, 1, 13)                0         ['dense_317[0][0]',           \n",
      "                                                                     'add_304[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_328 (L  (None, 1, 13)                26        ['add_305[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_153 (  (None, 1, 13)                225293    ['layer_normalization_328[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_328[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_306 (Add)               (None, 1, 13)                0         ['multi_head_attention_153[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_305[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_329 (L  (None, 1, 13)                26        ['add_306[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_318 (Dense)           (None, 1, 1024)              14336     ['layer_normalization_329[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_153 (Dropout)       (None, 1, 1024)              0         ['dense_318[0][0]']           \n",
      "                                                                                                  \n",
      " dense_319 (Dense)           (None, 1, 13)                13325     ['dropout_153[0][0]']         \n",
      "                                                                                                  \n",
      " add_307 (Add)               (None, 1, 13)                0         ['dense_319[0][0]',           \n",
      "                                                                     'add_306[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_330 (L  (None, 1, 13)                26        ['add_307[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_154 (  (None, 1, 13)                225293    ['layer_normalization_330[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_330[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_308 (Add)               (None, 1, 13)                0         ['multi_head_attention_154[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_307[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_331 (L  (None, 1, 13)                26        ['add_308[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_320 (Dense)           (None, 1, 1024)              14336     ['layer_normalization_331[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_154 (Dropout)       (None, 1, 1024)              0         ['dense_320[0][0]']           \n",
      "                                                                                                  \n",
      " dense_321 (Dense)           (None, 1, 13)                13325     ['dropout_154[0][0]']         \n",
      "                                                                                                  \n",
      " add_309 (Add)               (None, 1, 13)                0         ['dense_321[0][0]',           \n",
      "                                                                     'add_308[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_332 (L  (None, 1, 13)                26        ['add_309[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " multi_head_attention_155 (  (None, 1, 13)                225293    ['layer_normalization_332[0][0\n",
      " MultiHeadAttention)                                                ]',                           \n",
      "                                                                     'layer_normalization_332[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " add_310 (Add)               (None, 1, 13)                0         ['multi_head_attention_155[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'add_309[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_333 (L  (None, 1, 13)                26        ['add_310[0][0]']             \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_322 (Dense)           (None, 1, 1024)              14336     ['layer_normalization_333[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_155 (Dropout)       (None, 1, 1024)              0         ['dense_322[0][0]']           \n",
      "                                                                                                  \n",
      " dense_323 (Dense)           (None, 1, 13)                13325     ['dropout_155[0][0]']         \n",
      "                                                                                                  \n",
      " add_311 (Add)               (None, 1, 13)                0         ['dense_323[0][0]',           \n",
      "                                                                     'add_310[0][0]']             \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 13)                   0         ['add_311[0][0]']             \n",
      " 2 (GlobalAveragePooling1D)                                                                       \n",
      "                                                                                                  \n",
      " layer_normalization_334 (L  (None, 13)                   26        ['global_average_pooling1d_12[\n",
      " ayerNormalization)                                                 0][0]']                       \n",
      "                                                                                                  \n",
      " dense_324 (Dense)           (None, 1)                    14        ['layer_normalization_334[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3036112 (11.58 MB)\n",
      "Trainable params: 3036112 (11.58 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = train_sequences.shape[1:]\n",
    "head_size = 256\n",
    "num_heads = 16\n",
    "ff_dim = 1024\n",
    "num_layers = 12\n",
    "dropout = 0.20\n",
    "print(input_shape)\n",
    "\n",
    "model = build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_layers, dropout)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mae_loss(y_true, y_pred):\n",
    "    y_true_next = tf.cast(y_true[:, 1], tf.float64)  # Extract the true next values, scaled\n",
    "    y_pred_next = tf.cast(y_pred[:, 0], tf.float64)  # Extract the predicted next values, scaled\n",
    "    abs_error = tf.abs(y_true_next - y_pred_next)  # Calculate the absolute error\n",
    "    return tf.reduce_mean(abs_error)  # Return the mean of these errors\n",
    "\n",
    "def dir_acc(y_true, y_pred):\n",
    "    mean, std = tf.cast(y_true[:, 2], tf.float64), tf.cast(y_true[:, 3], tf.float64)  # Retrieve scaling factors\n",
    "    y_true_prev = (tf.cast(y_true[:, 0], tf.float64) * std) + mean  # Un-scale previous true price\n",
    "    y_true_next = (tf.cast(y_true[:, 1], tf.float64) * std) + mean  # Un-scale next true price\n",
    "    y_pred_next = (tf.cast(y_pred[:, 0], tf.float64) * std) + mean  # Un-scale predicted next price\n",
    "\n",
    "    true_change = y_true_next - y_true_prev  # Calculate true change\n",
    "    pred_change = y_pred_next - y_true_prev  # Calculate predicted change\n",
    "\n",
    "    correct_direction = tf.equal(tf.sign(true_change), tf.sign(pred_change))  # Check if the signs match\n",
    "    return tf.reduce_mean(tf.cast(correct_direction, tf.float64))  # Return the mean of correct directions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback to save the best model\n",
    "import matplotlib.pyplot as plt\n",
    "checkpoint_callback_train = ModelCheckpoint(\n",
    "    \"transformer_train_model.keras\",  # Filepath to save the best model\n",
    "    monitor=\"dir_acc\",  #\"loss\",  # Metric to monitor\n",
    "    save_best_only=True,  # Save only the best model\n",
    "    mode=\"max\",  # Minimize the monitored metric \n",
    "    verbose=1,  # Display progress\n",
    ")\n",
    "\n",
    "# Define a callback to save the best model\n",
    "checkpoint_callback_val = ModelCheckpoint(\n",
    "    \"transformer_val_model.keras\",  # Filepath to save the best model\n",
    "    monitor=\"val_dir_acc\", #\"val_loss\",  # Metric to monitor\n",
    "    save_best_only=True,  # Save only the best model\n",
    "    mode=\"max\",  # Minimize the monitored metric \n",
    "    verbose=1,  # Display progress\n",
    ")\n",
    "\n",
    "def get_lr_callback(batch_size=16, mode='cos', epochs=500, plot=False):\n",
    "    lr_start, lr_max, lr_min = 0.0001, 0.005, 0.00001  # Adjust learning rate boundaries\n",
    "    lr_ramp_ep = int(0.30 * epochs)  # 30% of epochs for warm-up\n",
    "    lr_sus_ep = max(0, int(0.10 * epochs) - lr_ramp_ep)  # Optional sustain phase, adjust as needed\n",
    "\n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:  # Warm-up phase\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:  # Sustain phase at max learning rate\n",
    "            lr = lr_max\n",
    "        elif mode == 'cos':\n",
    "            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep, epoch - lr_ramp_ep - lr_sus_ep\n",
    "            phase = math.pi * decay_epoch_index / decay_total_epochs\n",
    "            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n",
    "        else:\n",
    "            lr = lr_min  # Default to minimum learning rate if mode is not recognized\n",
    "\n",
    "        return lr\n",
    "\n",
    "    if plot:  # Plot learning rate curve if plot is True\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.title('Learning Rate Scheduler')\n",
    "        plt.show()\n",
    "\n",
    "    return tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "for i in range(len(train_labels_open)):\n",
    "    train_labels.append([train_labels_open[i]])\n",
    "    train_labels.append([train_labels_close[i]])\n",
    "train_labels = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 1/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.4823 - dir_acc: 0.4920WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 1: dir_acc improved from -inf to 0.49190, saving model to transformer_train_model.keras\n",
      "\n",
      "Epoch 1: val_dir_acc improved from -inf to 0.45307, saving model to transformer_val_model.keras\n",
      "136/136 [==============================] - 16s 64ms/step - loss: 0.4818 - dir_acc: 0.4919 - val_loss: 23.5715 - val_dir_acc: 0.4531 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.00026333333333333336.\n",
      "Epoch 2/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.3553 - dir_acc: 0.4933WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 2: dir_acc improved from 0.49190 to 0.49491, saving model to transformer_train_model.keras\n",
      "\n",
      "Epoch 2: val_dir_acc improved from 0.45307 to 0.47857, saving model to transformer_val_model.keras\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.3555 - dir_acc: 0.4949 - val_loss: 24.1418 - val_dir_acc: 0.4786 - lr: 2.6333e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.00042666666666666667.\n",
      "Epoch 3/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.3158 - dir_acc: 0.4959WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 3: dir_acc improved from 0.49491 to 0.49685, saving model to transformer_train_model.keras\n",
      "\n",
      "Epoch 3: val_dir_acc improved from 0.47857 to 0.49201, saving model to transformer_val_model.keras\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.3158 - dir_acc: 0.4969 - val_loss: 25.9868 - val_dir_acc: 0.4920 - lr: 4.2667e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.00059.\n",
      "Epoch 4/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.2910 - dir_acc: 0.4944WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 4: dir_acc did not improve from 0.49685\n",
      "\n",
      "Epoch 4: val_dir_acc did not improve from 0.49201\n",
      "136/136 [==============================] - 8s 61ms/step - loss: 0.2906 - dir_acc: 0.4954 - val_loss: 31.6342 - val_dir_acc: 0.4688 - lr: 5.9000e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0007533333333333334.\n",
      "Epoch 5/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.2584 - dir_acc: 0.4874WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 5: dir_acc did not improve from 0.49685\n",
      "\n",
      "Epoch 5: val_dir_acc improved from 0.49201 to 0.49396, saving model to transformer_val_model.keras\n",
      "136/136 [==============================] - 8s 61ms/step - loss: 0.2585 - dir_acc: 0.4877 - val_loss: 31.8394 - val_dir_acc: 0.4940 - lr: 7.5333e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0009166666666666668.\n",
      "Epoch 6/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.2404 - dir_acc: 0.4942WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 6: dir_acc did not improve from 0.49685\n",
      "\n",
      "Epoch 6: val_dir_acc did not improve from 0.49396\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.2402 - dir_acc: 0.4937 - val_loss: 23.6929 - val_dir_acc: 0.4706 - lr: 9.1667e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.00108.\n",
      "Epoch 7/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.2142 - dir_acc: 0.4976WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 7: dir_acc improved from 0.49685 to 0.49776, saving model to transformer_train_model.keras\n",
      "\n",
      "Epoch 7: val_dir_acc did not improve from 0.49396\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.2138 - dir_acc: 0.4978 - val_loss: 23.2636 - val_dir_acc: 0.4532 - lr: 0.0011\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0012433333333333335.\n",
      "Epoch 8/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.2024 - dir_acc: 0.4891WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 8: dir_acc did not improve from 0.49776\n",
      "\n",
      "Epoch 8: val_dir_acc improved from 0.49396 to 0.50385, saving model to transformer_val_model.keras\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.2024 - dir_acc: 0.4908 - val_loss: 20.7044 - val_dir_acc: 0.5038 - lr: 0.0012\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0014066666666666667.\n",
      "Epoch 9/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.2026 - dir_acc: 0.4973WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 9: dir_acc did not improve from 0.49776\n",
      "\n",
      "Epoch 9: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.2024 - dir_acc: 0.4972 - val_loss: 31.7893 - val_dir_acc: 0.4669 - lr: 0.0014\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.00157.\n",
      "Epoch 10/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.2090 - dir_acc: 0.4992WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 10: dir_acc did not improve from 0.49776\n",
      "\n",
      "Epoch 10: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 61ms/step - loss: 0.2089 - dir_acc: 0.4976 - val_loss: 26.4617 - val_dir_acc: 0.4805 - lr: 0.0016\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.0017333333333333335.\n",
      "Epoch 11/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1913 - dir_acc: 0.4959WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 11: dir_acc did not improve from 0.49776\n",
      "\n",
      "Epoch 11: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.1913 - dir_acc: 0.4955 - val_loss: 27.9812 - val_dir_acc: 0.4709 - lr: 0.0017\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.0018966666666666667.\n",
      "Epoch 12/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1945 - dir_acc: 0.4988WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 12: dir_acc improved from 0.49776 to 0.49833, saving model to transformer_train_model.keras\n",
      "\n",
      "Epoch 12: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 9s 67ms/step - loss: 0.1947 - dir_acc: 0.4983 - val_loss: 29.4198 - val_dir_acc: 0.4920 - lr: 0.0019\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.0020599999999999998.\n",
      "Epoch 13/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1691 - dir_acc: 0.5012WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 13: dir_acc improved from 0.49833 to 0.50202, saving model to transformer_train_model.keras\n",
      "\n",
      "Epoch 13: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 61ms/step - loss: 0.1691 - dir_acc: 0.5020 - val_loss: 34.9338 - val_dir_acc: 0.4746 - lr: 0.0021\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.0022233333333333332.\n",
      "Epoch 14/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1737 - dir_acc: 0.5016WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 14: dir_acc did not improve from 0.50202\n",
      "\n",
      "Epoch 14: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.1735 - dir_acc: 0.4997 - val_loss: 24.7470 - val_dir_acc: 0.4493 - lr: 0.0022\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.0023866666666666667.\n",
      "Epoch 15/100\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1926 - dir_acc: 0.4982WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 15: dir_acc did not improve from 0.50202\n",
      "\n",
      "Epoch 15: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.1926 - dir_acc: 0.4982 - val_loss: 24.8355 - val_dir_acc: 0.4708 - lr: 0.0024\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.0025499999999999997.\n",
      "Epoch 16/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.2035 - dir_acc: 0.5044WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 16: dir_acc improved from 0.50202 to 0.50524, saving model to transformer_train_model.keras\n",
      "\n",
      "Epoch 16: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.2033 - dir_acc: 0.5052 - val_loss: 29.2965 - val_dir_acc: 0.4825 - lr: 0.0026\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.0027133333333333332.\n",
      "Epoch 17/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1875 - dir_acc: 0.5006WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 17: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 17: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.1877 - dir_acc: 0.5014 - val_loss: 28.1648 - val_dir_acc: 0.4631 - lr: 0.0027\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.0028766666666666667.\n",
      "Epoch 18/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.2022 - dir_acc: 0.4919WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 18: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 18: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 58ms/step - loss: 0.2022 - dir_acc: 0.4925 - val_loss: 25.9659 - val_dir_acc: 0.4901 - lr: 0.0029\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.0030399999999999997.\n",
      "Epoch 19/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1876 - dir_acc: 0.4936WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 19: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 19: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.1876 - dir_acc: 0.4939 - val_loss: 20.0921 - val_dir_acc: 0.4786 - lr: 0.0030\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.003203333333333333.\n",
      "Epoch 20/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1682 - dir_acc: 0.4925WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 20: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 20: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.1681 - dir_acc: 0.4924 - val_loss: 28.1857 - val_dir_acc: 0.4804 - lr: 0.0032\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.0033666666666666667.\n",
      "Epoch 21/100\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1861 - dir_acc: 0.4990WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 21: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 21: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.1861 - dir_acc: 0.4990 - val_loss: 30.3554 - val_dir_acc: 0.4882 - lr: 0.0034\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.0035299999999999997.\n",
      "Epoch 22/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1687 - dir_acc: 0.4988WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 22: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 22: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 59ms/step - loss: 0.1687 - dir_acc: 0.4983 - val_loss: 26.7980 - val_dir_acc: 0.4726 - lr: 0.0035\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.003693333333333333.\n",
      "Epoch 23/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1710 - dir_acc: 0.5038WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 23: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 23: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.1708 - dir_acc: 0.5040 - val_loss: 29.2337 - val_dir_acc: 0.4825 - lr: 0.0037\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.0038566666666666667.\n",
      "Epoch 24/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1583 - dir_acc: 0.5023WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 24: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 24: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.1584 - dir_acc: 0.5018 - val_loss: 27.8760 - val_dir_acc: 0.4843 - lr: 0.0039\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.00402.\n",
      "Epoch 25/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1760 - dir_acc: 0.5016WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 25: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 25: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.1761 - dir_acc: 0.5025 - val_loss: 23.8486 - val_dir_acc: 0.4881 - lr: 0.0040\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.004183333333333334.\n",
      "Epoch 26/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1503 - dir_acc: 0.4872WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 26: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 26: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.1502 - dir_acc: 0.4878 - val_loss: 23.1367 - val_dir_acc: 0.4804 - lr: 0.0042\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.004346666666666667.\n",
      "Epoch 27/100\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.1465 - dir_acc: 0.5051WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 27: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 27: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 61ms/step - loss: 0.1465 - dir_acc: 0.5051 - val_loss: 27.1676 - val_dir_acc: 0.4650 - lr: 0.0043\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.00451.\n",
      "Epoch 28/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1609 - dir_acc: 0.4942WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 28: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 28: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 61ms/step - loss: 0.1608 - dir_acc: 0.4937 - val_loss: 28.5818 - val_dir_acc: 0.4785 - lr: 0.0045\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.004673333333333334.\n",
      "Epoch 29/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1579 - dir_acc: 0.5008WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 29: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 29: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.1579 - dir_acc: 0.5010 - val_loss: 27.7383 - val_dir_acc: 0.4688 - lr: 0.0047\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.004836666666666667.\n",
      "Epoch 30/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1681 - dir_acc: 0.5035WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 30: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 30: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 62ms/step - loss: 0.1681 - dir_acc: 0.5033 - val_loss: 27.0537 - val_dir_acc: 0.4631 - lr: 0.0048\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.005.\n",
      "Epoch 31/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1678 - dir_acc: 0.5012WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 31: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 31: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 61ms/step - loss: 0.1677 - dir_acc: 0.4999 - val_loss: 30.5579 - val_dir_acc: 0.4728 - lr: 0.0050\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.00499748770102058.\n",
      "Epoch 32/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1529 - dir_acc: 0.5019WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 32: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 32: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 62ms/step - loss: 0.1527 - dir_acc: 0.5031 - val_loss: 30.4717 - val_dir_acc: 0.4727 - lr: 0.0050\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.0049899558635181215.\n",
      "Epoch 33/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1475 - dir_acc: 0.5027WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 33: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 33: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 62ms/step - loss: 0.1474 - dir_acc: 0.5028 - val_loss: 29.9114 - val_dir_acc: 0.4825 - lr: 0.0050\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.004977419655610997.\n",
      "Epoch 34/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1573 - dir_acc: 0.4957WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 34: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 34: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 62ms/step - loss: 0.1573 - dir_acc: 0.4959 - val_loss: 32.4638 - val_dir_acc: 0.4843 - lr: 0.0050\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.004959904323553581.\n",
      "Epoch 35/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1417 - dir_acc: 0.4976WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 35: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 35: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 62ms/step - loss: 0.1416 - dir_acc: 0.4988 - val_loss: 32.4563 - val_dir_acc: 0.4727 - lr: 0.0050\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.0049374451408936496.\n",
      "Epoch 36/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1449 - dir_acc: 0.5029WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 36: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 36: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 60ms/step - loss: 0.1451 - dir_acc: 0.5041 - val_loss: 29.4333 - val_dir_acc: 0.4786 - lr: 0.0049\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 0.004910087337436154.\n",
      "Epoch 37/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1348 - dir_acc: 0.5012WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 37: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 37: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 9s 65ms/step - loss: 0.1349 - dir_acc: 0.5006 - val_loss: 29.2063 - val_dir_acc: 0.4825 - lr: 0.0049\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 0.004877886008156408.\n",
      "Epoch 38/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1455 - dir_acc: 0.5010WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 38: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 38: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 9s 63ms/step - loss: 0.1453 - dir_acc: 0.5002 - val_loss: 28.4021 - val_dir_acc: 0.4921 - lr: 0.0049\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 0.004840906002246144.\n",
      "Epoch 39/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1344 - dir_acc: 0.4957WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 39: dir_acc did not improve from 0.50524\n",
      "\n",
      "Epoch 39: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 8s 62ms/step - loss: 0.1342 - dir_acc: 0.4952 - val_loss: 25.8440 - val_dir_acc: 0.4786 - lr: 0.0048\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 0.00479922179251587.\n",
      "Epoch 40/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1308 - dir_acc: 0.5086WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 40: dir_acc improved from 0.50524 to 0.50833, saving model to transformer_train_model.keras\n",
      "\n",
      "Epoch 40: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 9s 68ms/step - loss: 0.1314 - dir_acc: 0.5083 - val_loss: 26.0479 - val_dir_acc: 0.4902 - lr: 0.0048\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 0.0047529173254165355.\n",
      "Epoch 41/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1483 - dir_acc: 0.5057WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 41: dir_acc did not improve from 0.50833\n",
      "\n",
      "Epoch 41: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 9s 63ms/step - loss: 0.1483 - dir_acc: 0.5051 - val_loss: 26.6388 - val_dir_acc: 0.4804 - lr: 0.0048\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 0.004702085851982562.\n",
      "Epoch 42/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1185 - dir_acc: 0.5054WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 42: dir_acc did not improve from 0.50833\n",
      "\n",
      "Epoch 42: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 9s 64ms/step - loss: 0.1184 - dir_acc: 0.5073 - val_loss: 28.4633 - val_dir_acc: 0.4921 - lr: 0.0047\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 0.004646829740036656.\n",
      "Epoch 43/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1417 - dir_acc: 0.5010WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 43: dir_acc did not improve from 0.50833\n",
      "\n",
      "Epoch 43: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 9s 69ms/step - loss: 0.1420 - dir_acc: 0.5012 - val_loss: 20.7963 - val_dir_acc: 0.4689 - lr: 0.0046\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 0.00458726026803465.\n",
      "Epoch 44/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1202 - dir_acc: 0.5012WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 44: dir_acc did not improve from 0.50833\n",
      "\n",
      "Epoch 44: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 9s 66ms/step - loss: 0.1203 - dir_acc: 0.5006 - val_loss: 27.0654 - val_dir_acc: 0.4804 - lr: 0.0046\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 0.004523497400965494.\n",
      "Epoch 45/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1214 - dir_acc: 0.5057WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 45: dir_acc did not improve from 0.50833\n",
      "\n",
      "Epoch 45: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 9s 68ms/step - loss: 0.1215 - dir_acc: 0.5055 - val_loss: 28.0113 - val_dir_acc: 0.4766 - lr: 0.0045\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 0.004455669548757734.\n",
      "Epoch 46/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1195 - dir_acc: 0.4936WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 46: dir_acc did not improve from 0.50833\n",
      "\n",
      "Epoch 46: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.1194 - dir_acc: 0.4925 - val_loss: 28.0066 - val_dir_acc: 0.4747 - lr: 0.0045\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 0.00438391330767901.\n",
      "Epoch 47/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1089 - dir_acc: 0.4980WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 47: dir_acc did not improve from 0.50833\n",
      "\n",
      "Epoch 47: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 11s 78ms/step - loss: 0.1088 - dir_acc: 0.4986 - val_loss: 31.7030 - val_dir_acc: 0.4920 - lr: 0.0044\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 0.004308373185249342.\n",
      "Epoch 48/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1174 - dir_acc: 0.5128WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 48: dir_acc improved from 0.50833 to 0.51328, saving model to transformer_train_model.keras\n",
      "\n",
      "Epoch 48: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 10s 72ms/step - loss: 0.1173 - dir_acc: 0.5133 - val_loss: 27.5785 - val_dir_acc: 0.4863 - lr: 0.0043\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 0.004229201309222228.\n",
      "Epoch 49/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.0971 - dir_acc: 0.5058WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 49: dir_acc did not improve from 0.51328\n",
      "\n",
      "Epoch 49: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 10s 71ms/step - loss: 0.0971 - dir_acc: 0.5059 - val_loss: 30.2960 - val_dir_acc: 0.4940 - lr: 0.0042\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 0.0041465571212195825.\n",
      "Epoch 50/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1125 - dir_acc: 0.5073WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 50: dir_acc did not improve from 0.51328\n",
      "\n",
      "Epoch 50: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 10s 75ms/step - loss: 0.1126 - dir_acc: 0.5081 - val_loss: 29.8566 - val_dir_acc: 0.4805 - lr: 0.0041\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 0.0040606070556375405.\n",
      "Epoch 51/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1091 - dir_acc: 0.5081WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 51: dir_acc did not improve from 0.51328\n",
      "\n",
      "Epoch 51: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 10s 73ms/step - loss: 0.1092 - dir_acc: 0.5082 - val_loss: 27.9540 - val_dir_acc: 0.4747 - lr: 0.0041\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 0.00397152420446972.\n",
      "Epoch 52/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1000 - dir_acc: 0.5078WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 52: dir_acc did not improve from 0.51328\n",
      "\n",
      "Epoch 52: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 10s 74ms/step - loss: 0.1000 - dir_acc: 0.5072 - val_loss: 29.9918 - val_dir_acc: 0.4785 - lr: 0.0040\n",
      "\n",
      "Epoch 53: LearningRateScheduler setting learning rate to 0.0038794879687229964.\n",
      "Epoch 53/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.0997 - dir_acc: 0.5027WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 53: dir_acc did not improve from 0.51328\n",
      "\n",
      "Epoch 53: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 11s 84ms/step - loss: 0.0996 - dir_acc: 0.5035 - val_loss: 25.3920 - val_dir_acc: 0.4863 - lr: 0.0039\n",
      "\n",
      "Epoch 54: LearningRateScheduler setting learning rate to 0.0037846836971277367.\n",
      "Epoch 54/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.1053 - dir_acc: 0.5058WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 54: dir_acc did not improve from 0.51328\n",
      "\n",
      "Epoch 54: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 12s 89ms/step - loss: 0.1053 - dir_acc: 0.5066 - val_loss: 27.9318 - val_dir_acc: 0.4765 - lr: 0.0038\n",
      "\n",
      "Epoch 55: LearningRateScheduler setting learning rate to 0.003687302312870132.\n",
      "Epoch 55/100\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0920 - dir_acc: 0.5024WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 55: dir_acc did not improve from 0.51328\n",
      "\n",
      "Epoch 55: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 12s 90ms/step - loss: 0.0920 - dir_acc: 0.5024 - val_loss: 26.9252 - val_dir_acc: 0.4824 - lr: 0.0037\n",
      "\n",
      "Epoch 56: LearningRateScheduler setting learning rate to 0.0035875399290983077.\n",
      "Epoch 56/100\n",
      "136/136 [==============================] - ETA: 0s - loss: 0.0898 - dir_acc: 0.5043WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 56: dir_acc did not improve from 0.51328\n",
      "\n",
      "Epoch 56: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 13s 93ms/step - loss: 0.0898 - dir_acc: 0.5043 - val_loss: 30.7242 - val_dir_acc: 0.4920 - lr: 0.0036\n",
      "\n",
      "Epoch 57: LearningRateScheduler setting learning rate to 0.00348559745397654.\n",
      "Epoch 57/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.0993 - dir_acc: 0.5118WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 57: dir_acc did not improve from 0.51328\n",
      "\n",
      "Epoch 57: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 12s 85ms/step - loss: 0.0992 - dir_acc: 0.5122 - val_loss: 27.2340 - val_dir_acc: 0.4843 - lr: 0.0035\n",
      "\n",
      "Epoch 58: LearningRateScheduler setting learning rate to 0.0033816801860829505.\n",
      "Epoch 58/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.0996 - dir_acc: 0.5102WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 58: dir_acc did not improve from 0.51328\n",
      "\n",
      "Epoch 58: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 12s 89ms/step - loss: 0.0996 - dir_acc: 0.5103 - val_loss: 27.2404 - val_dir_acc: 0.4650 - lr: 0.0034\n",
      "\n",
      "Epoch 59: LearningRateScheduler setting learning rate to 0.0032759974009654944.\n",
      "Epoch 59/100\n",
      "135/136 [============================>.] - ETA: 0s - loss: 0.0888 - dir_acc: 0.5071WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 59: dir_acc did not improve from 0.51328\n",
      "\n",
      "Epoch 59: val_dir_acc did not improve from 0.50385\n",
      "136/136 [==============================] - 13s 95ms/step - loss: 0.0887 - dir_acc: 0.5068 - val_loss: 26.4283 - val_dir_acc: 0.4882 - lr: 0.0033\n",
      "\n",
      "Epoch 60: LearningRateScheduler setting learning rate to 0.0031687619296888545.\n",
      "Epoch 60/100\n",
      " 22/136 [===>..........................] - ETA: 8s - loss: 0.0865 - dir_acc: 0.5078"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64  # Number of training examples used to calculate each iteration's gradient\n",
    "EPOCHS = 100  # Total number of times the entire dataset is passed through the network\n",
    "model.compile(\n",
    "    optimizer=\"adam\",  # Optimizer to use\n",
    "    loss=custom_mae_loss,  # Loss function to minimize\n",
    "    metrics=[dir_acc]  # Metric to track\n",
    ")\n",
    "model.fit(\n",
    "    train_sequences,  # Training features\n",
    "    train_labels_open + train_labels_close,  # Training labels\n",
    "    validation_data=(validation_sequences, validation_labels_open, validation_labels_close),  # Validation data\n",
    "    epochs=EPOCHS,  # Number of epochs to train for\n",
    "    batch_size=BATCH_SIZE,  # Size of each batch\n",
    "    shuffle=True,  # Shuffle training data before each epoch\n",
    "    callbacks=[checkpoint_callback_train, checkpoint_callback_val, get_lr_callback(batch_size=BATCH_SIZE, epochs=EPOCHS)]  # Callbacks for saving models and adjusting learning rate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[345], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m r2_score\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming model is already defined and weights are loaded\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels_close\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_sequences)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:272\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m _check_data_cardinality(inputs)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# If batch_size is not passed but steps is, calculate from the input\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# data.  Defaults to `32` for backwards compatibility.\u001b[39;00m\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch_size:\n\u001b[1;32m    273\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39mceil(num_samples \u001b[38;5;241m/\u001b[39m steps)) \u001b[38;5;28;01mif\u001b[39;00m steps \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(math\u001b[38;5;241m.\u001b[39mceil(num_samples \u001b[38;5;241m/\u001b[39m batch_size))\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"transformer_val_model.keras\")  # Load the best model from the validation phase\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Assuming model is already defined and weights are loaded\n",
    "accuracy = model.evaluate(test_sequences, test_labels_open + test_labels_close)[1]\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "predictions = model.predict(test_sequences)\n",
    "\n",
    "# Calculate R-squared value\n",
    "r2_open = r2_score(test_labels_open[:, 1], predictions[:, 0])\n",
    "r2_close = r2_score(test_labels_close[:, 1], predictions[:, 0])\n",
    "print(f\"R-squared: {r2_close}\" )\n",
    "print(f\"R-squared: {r2_open}\")\n",
    "\n",
    "# Plot actual vs. predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(test_labels_open[:, 1], predictions[:, 0], alpha=0.5)\n",
    "# plt.scatter(test_labels_close[:, 1], predictions[:, 0], alpha=0.5)\n",
    "# plt.plot([min(test_labels_open[:, 1]), max(test_labels_open[:, 1])],\n",
    "#          [min(test_labels_close[:, 1]), max(test_labels_close[:, 1])], color='red', linestyle='--')\n",
    "# plt.xlabel('Actual Values')\n",
    "# plt.ylabel('Predicted Values')\n",
    "plt.plot(test_labels_open[:,1])\n",
    "plt.plot( predictions[:, 0])\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
